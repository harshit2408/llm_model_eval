{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_client(api_key):\n",
    "    return OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query the LLM and get both reasoning and Yes/No answer\n",
    "def query_llm(client, model_name, paragraph, question):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Paragraph: \\\"{paragraph}\\\"\\nQuestion: \\\"{question}\\\"\\nAnswer strictly in 'Yes' or 'No', followed by reasoning.\"\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#using regex to match yes/no at the start or in the middle\n",
    "def extract_answer_and_reasoning(response):\n",
    "    if not response:\n",
    "        return None, None\n",
    "\n",
    "    answer_match = re.search(r\"\\b(yes|no)\\b\", response, re.IGNORECASE)\n",
    "    reasoning = None\n",
    "\n",
    "    if answer_match:\n",
    "        answer = answer_match.group(1).lower() \n",
    "        \n",
    "        reasoning_start = response.lower().find(answer) + len(answer)\n",
    "        raw_reasoning = response[reasoning_start:].strip()\n",
    "\n",
    "        lines = raw_reasoning.splitlines()\n",
    "        filtered_lines = [line.strip() for line in lines if line.strip() not in (\"**\", \"\")]\n",
    "\n",
    "        cleaned_lines = [re.sub(r\"^reasoning:\\s*\", \"\", line, flags=re.IGNORECASE) for line in filtered_lines]\n",
    "\n",
    "        reasoning = \" \".join(cleaned_lines).strip()\n",
    "    else:\n",
    "        answer = \"invalid\"\n",
    "\n",
    "    return answer, reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ambiguity(client, model_name, paragraph, question):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Paragraph: \\\"{paragraph}\\\"\\nQuestion: \\\"{question}\\\"\\nClassify the question's clarity in relation to the paragraph as 'Clear' or 'Ambiguous'. Provide reasoning for your classification.\"\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        label_match = re.search(r\"\\b(clear|ambiguous)\\b\", result, re.IGNORECASE)\n",
    "        label = label_match.group(1).lower() if label_match else \"invalid\"\n",
    "        reasoning = result[len(label_match.group(0)) + 1:].strip() if label_match else None\n",
    "        return label, reasoning\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating ambiguity: {e}\")\n",
    "        return \"invalid\", None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_explanation_quality(client, model_name, paragraph, reasoning):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Paragraph: \\\"{paragraph}\\\"\\nReasoning: \\\"{reasoning}\\\"\\nRate the quality of the reasoning on a scale of 1 to 5 (1: Poor, 5: Excellent). Provide a brief justification for the rating.\"\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        grade_match = re.search(r\"\\b[1-5]\\b\", result)\n",
    "        grade = int(grade_match.group(0)) if grade_match else -1\n",
    "        justification = result[grade_match.end():].strip() if grade_match else None\n",
    "        return grade, justification\n",
    "    except Exception as e:\n",
    "        print(f\"Error grading explanation quality: {e}\")\n",
    "        return -1, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(models, dataset):\n",
    "    all_results = []\n",
    "\n",
    "    for model_info in models:\n",
    "        api_key = model_info[\"api_key\"]\n",
    "        model_name = model_info[\"model_name\"]\n",
    "        \n",
    "        client = initialize_client(api_key)\n",
    "        print(f\"Evaluating model: {model_name}\")\n",
    "\n",
    "        for entry in dataset:\n",
    "            paragraph = entry[\"paragraph\"]\n",
    "            for question_entry in entry[\"questions\"]:\n",
    "                question = question_entry[\"question\"]\n",
    "                ground_truth = question_entry[\"ground_truth\"].lower()\n",
    "\n",
    "                llm_response = query_llm(client, model_name, paragraph, question)\n",
    "                llm_answer, reasoning = extract_answer_and_reasoning(llm_response)\n",
    "\n",
    "                ambiguity_label, ambiguity_reasoning = evaluate_ambiguity(client, model_name, paragraph, question)\n",
    "\n",
    "                quality_grade, quality_justification = grade_explanation_quality(client, model_name, paragraph, reasoning)\n",
    "\n",
    "                is_correct = llm_answer == ground_truth\n",
    "\n",
    "                all_results.append({\n",
    "                    \"model_name\": model_name,\n",
    "                    \"paragraph\": paragraph,\n",
    "                    \"question\": question,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                    \"llm_answer\": llm_answer,\n",
    "                    \"reasoning\": reasoning,\n",
    "                    \"ambiguity_label\": ambiguity_label,\n",
    "                    \"ambiguity_reasoning\": ambiguity_reasoning,\n",
    "                    \"quality_grade\": quality_grade,\n",
    "                    \"quality_justification\": quality_justification,\n",
    "                    \"is_correct\": is_correct,\n",
    "                })\n",
    "\n",
    "                print(f\"Paragraph: {paragraph}\")\n",
    "                print(f\"Question: {question}\")\n",
    "                print(f\"Ground Truth: {ground_truth}\")\n",
    "                print(f\"LLM Answer: {llm_answer}\")\n",
    "                print(f\"Reasoning: {reasoning}\")\n",
    "                print(f\"Ambiguity Label: {ambiguity_label}\")\n",
    "                print(f\"Ambiguity Reasoning: {ambiguity_reasoning}\")\n",
    "                print(f\"Quality Grade: {quality_grade}\")\n",
    "                print(f\"Quality Justification: {quality_justification}\")\n",
    "                print(f\"Correct: {is_correct}\\n\")\n",
    "\n",
    "    return pd.DataFrame(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    {\n",
    "        \"paragraph\": \"The company launched its new software with features like data encryption and automatic backups. Pricing information is available, but there's no mention of customer support options.\",\n",
    "        \"questions\": [\n",
    "            {\n",
    "                \"question\": \"Does it mention pricing?\",\n",
    "                \"ground_truth\": \"yes\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"Is customer support discussed?\",\n",
    "                \"ground_truth\": \"no\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"Does it talk about data encryption?\",\n",
    "                \"ground_truth\": \"yes\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"paragraph\": \"The smartphone boasts a 6.5-inch display and comes with a dual-lens camera system. However, the battery capacity and processor details were not revealed.\",\n",
    "        \"questions\": [\n",
    "            {\n",
    "                \"question\": \"Is the display size mentioned?\",\n",
    "                \"ground_truth\": \"yes\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"Does it discuss battery capacity?\",\n",
    "                \"ground_truth\": \"no\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"Are camera features included?\",\n",
    "                \"ground_truth\": \"yes\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\n",
    "        \"model_name\": \"meta-llama/llama-3.2-3b-instruct:free\",\n",
    "        \"api_key\": \"ap_key_1\",  \n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "        \"api_key\": \"api_key_2\", \n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: meta-llama/llama-3.2-3b-instruct:free\n",
      "Paragraph: The company launched its new software with features like data encryption and automatic backups. Pricing information is available, but there's no mention of customer support options.\n",
      "Question: Does it mention pricing?\n",
      "Ground Truth: yes\n",
      "LLM Answer: no\n",
      "Reasoning: , it does not mention pricing. The text states \"Pricing information is available\", which implies that it is mentioned, but does not provide the details itself.\n",
      "Ambiguity Label: ambiguous\n",
      "Ambiguity Reasoning: assify the question's clarity as 'Ambiguous'.\n",
      "\n",
      "The question asks if \"it mentions pricing\", but the paragraph specifically states that \"Pricing information is available\", which clearly mentions the existence of pricing information. The question is ambiguous because it doesn't specify what \"it\" refers to, which could be the software, the company, or something else. To answer the question accurately, it would be necessary to know what \"it\" refers to in the context of the paragraph.\n",
      "Quality Grade: 4\n",
      "Quality Justification: out of 5.\n",
      "\n",
      "The reasoning is excellent because it correctly identifies the implication of the text. The phrase \"Pricing information is available\" is often used to indicate that the information is available, but not necessarily provided in the text itself. This is a common linguistic trick used to convey that the information is available, but not explicitly stated. The reasoning accurately captures this nuance and provides a clear justification for the conclusion.\n",
      "\n",
      "The only reason I wouldn't give it a perfect score is that the reasoning is quite straightforward and doesn't require any complex analysis or interpretation. The conclusion is relatively obvious and can be inferred with minimal effort. Nevertheless, the reasoning is clear, concise, and accurate, making it a strong example of logical reasoning.\n",
      "Correct: False\n",
      "\n",
      "Paragraph: The company launched its new software with features like data encryption and automatic backups. Pricing information is available, but there's no mention of customer support options.\n",
      "Question: Is customer support discussed?\n",
      "Ground Truth: no\n",
      "LLM Answer: no\n",
      "Reasoning: , The paragraph states that pricing information is available, but there's no mention of customer support options.\n",
      "Ambiguity Label: ambiguous\n",
      "Ambiguity Reasoning: assify the question's clarity as 'Ambiguous'.\n",
      "\n",
      "The question asks if customer support is discussed, but the paragraph actually mentions that there is no mention of customer support options. This implies that customer support is not discussed at all, rather than that it is discussed. The question's phrasing is too broad and does not accurately reflect the content of the paragraph.\n",
      "Quality Grade: 4\n",
      "Quality Justification: out of 5.\n",
      "\n",
      "The reasoning is clear and concise, and it directly addresses the information provided in the paragraph. The statement \"The paragraph states that pricing information is available, but there's no mention of customer support options\" effectively summarizes the main point of the paragraph. However, the reasoning could be slightly improved by providing more context or explanation about why the lack of customer support options is significant or noteworthy. As it stands, the reasoning is straightforward and to the point, but it doesn't offer much additional insight or analysis.\n",
      "Correct: True\n",
      "\n",
      "Paragraph: The company launched its new software with features like data encryption and automatic backups. Pricing information is available, but there's no mention of customer support options.\n",
      "Question: Does it talk about data encryption?\n",
      "Ground Truth: yes\n",
      "LLM Answer: yes\n",
      "Reasoning: . The paragraph mentions data encryption as one of the features of the new software.\n",
      "Ambiguity Label: clear\n",
      "Ambiguity Reasoning: d classify the question's clarity as 'Clear'. \n",
      "\n",
      "The reason for this classification is that the question directly references a specific feature mentioned in the paragraph, which is \"data encryption\". The question is straightforward and to the point, asking if the paragraph talks about data encryption, without any ambiguity or room for misinterpretation.\n",
      "Quality Grade: 4\n",
      "Quality Justification: .\n",
      "\n",
      "The reasoning is excellent because it directly addresses the information provided in the paragraph. The paragraph explicitly mentions data encryption as one of the features of the new software, and the reasoning correctly identifies this feature. The reasoning is also concise and to the point, making it easy to understand.\n",
      "\n",
      "The only reason I wouldn't give it a 5 is that the reasoning doesn't provide any additional analysis or insights beyond simply identifying the feature mentioned in the paragraph. However, given the straightforward nature of the information, this is a minor quibble.\n",
      "Correct: True\n",
      "\n",
      "Paragraph: The smartphone boasts a 6.5-inch display and comes with a dual-lens camera system. However, the battery capacity and processor details were not revealed.\n",
      "Question: Is the display size mentioned?\n",
      "Ground Truth: yes\n",
      "LLM Answer: yes\n",
      "Reasoning: . The display size is mentioned as \"6.5-inch\".\n",
      "Ambiguity Label: clear\n",
      "Ambiguity Reasoning: d classify the question's clarity as 'Clear'. \n",
      "\n",
      "The question directly asks if the display size is mentioned in the paragraph, and the answer to this question can be easily determined by reading the paragraph. The paragraph explicitly states that the smartphone has a 6.5-inch display, which directly answers the question.\n",
      "Quality Grade: 4\n",
      "Quality Justification: out of 5.\n",
      "\n",
      "The reasoning is clear and concise, and it directly addresses the specific detail mentioned in the paragraph. The phrase \". The display size is mentioned as '6.5-inch'\" is a good way to highlight the relevant information. However, the reasoning could be improved by providing more context or explaining why the display size is significant or relevant to the overall description of the smartphone. As it stands, the reasoning is somewhat superficial and doesn't add much depth or insight to the analysis.\n",
      "Correct: True\n",
      "\n",
      "Paragraph: The smartphone boasts a 6.5-inch display and comes with a dual-lens camera system. However, the battery capacity and processor details were not revealed.\n",
      "Question: Does it discuss battery capacity?\n",
      "Ground Truth: no\n",
      "LLM Answer: no\n",
      "Reasoning: , The paragraph does not explicitly discuss battery capacity, but rather mentions that the details were not revealed.\n",
      "Ambiguity Label: clear\n",
      "Ambiguity Reasoning: d classify the question's clarity as 'Clear'.\n",
      "\n",
      "The question is straightforward and specifically asks if the paragraph discusses battery capacity, which is a specific detail mentioned in the paragraph. The question does not contain any ambiguous language or require any inference, making it easy to understand and answer based on the provided paragraph.\n",
      "Quality Grade: 4\n",
      "Quality Justification: out of 5.\n",
      "\n",
      "The reasoning is excellent because it accurately identifies the lack of explicit discussion about battery capacity in the given paragraph. The phrase \"The details were not revealed\" implies that the information about battery capacity was not explicitly mentioned, rather than being absent or unknown. This shows a clear understanding of the text and the ability to extract relevant information.\n",
      "\n",
      "The only reason I wouldn't give it a perfect score is that the reasoning could be more explicit. For example, the statement \"The paragraph does not explicitly discuss battery capacity\" could be rephrased as \"The paragraph does not mention battery capacity explicitly.\" However, the current statement is still clear and effective, making it a 4 out of 5.\n",
      "Correct: True\n",
      "\n",
      "Paragraph: The smartphone boasts a 6.5-inch display and comes with a dual-lens camera system. However, the battery capacity and processor details were not revealed.\n",
      "Question: Are camera features included?\n",
      "Ground Truth: yes\n",
      "LLM Answer: yes\n",
      "Reasoning: . The paragraph specifically mentions that the smartphone comes with a dual-lens camera system, which clearly includes camera features.\n",
      "Ambiguity Label: ambiguous\n",
      "Ambiguity Reasoning: assify the question as 'Ambiguous'.\n",
      "\n",
      "The question is asking if camera features are included, but the paragraph only mentions that the smartphone comes with a \"dual-lens camera system\", which is a specific example of camera features, not a general statement that all camera features are included. This makes the question unclear, as it does not provide enough information to answer whether all camera features are included or not. A more precise question would be \"Are all camera features included?\" or \"Are any camera features missing?\" to clarify the intended meaning.\n",
      "Quality Grade: 4\n",
      "Quality Justification: .\n",
      "\n",
      "The reasoning is excellent because it identifies a specific piece of information in the paragraph that can be used to make an inference. The fact that the smartphone comes with a dual-lens camera system provides a clear indication that the camera features are present. The reasoning is clear and concise, and it directly addresses the question of whether the paragraph \"clearly includes camera features\".\n",
      "\n",
      "The only reason I wouldn't give it a 5 (Excellent) is that the reasoning is somewhat self-evident, and it doesn't really require much critical thinking or analysis. Anyone reading the paragraph would likely make the same inference without needing to apply any complex reasoning skills.\n",
      "Correct: True\n",
      "\n",
      "Evaluating model: nvidia/llama-3.1-nemotron-70b-instruct\n",
      "Paragraph: The company launched its new software with features like data encryption and automatic backups. Pricing information is available, but there's no mention of customer support options.\n",
      "Question: Does it mention pricing?\n",
      "Ground Truth: yes\n",
      "LLM Answer: yes\n",
      "Reasoning: The paragraph explicitly states \"Pricing information is available\", indicating that pricing details are indeed mentioned, even if the specific prices themselves are not provided.\n",
      "Ambiguity Label: clear\n",
      "Ambiguity Reasoning: d classify the question's clarity in relation to the paragraph as **'Clear'**. Here's my reasoning:\n",
      "\n",
      "1. **Direct relevance**: The question directly relates to information presented in the paragraph. The paragraph explicitly mentions \"Pricing information is available\", which is exactly what the question inquires about.\n",
      "2. **Unambiguous terminology**: The term \"pricing\" is used in both the paragraph and the question, eliminating potential confusion that might arise from synonyms or related but distinct concepts.\n",
      "3. **Straightforward answer**: Based on the paragraph, a definitive \"yes\" or \"no\" answer can be provided to the question (\"Does it mention pricing?\"). In this case, the answer would be \"Yes, it does mention pricing.\"\n",
      "\n",
      "Given these factors, the question's clarity in relation to the paragraph is straightforward, leaving little to no room for interpretation or confusion.\n",
      "Quality Grade: 5\n",
      "Quality Justification: (Excellent)**\n",
      "\n",
      "**Justification:**\n",
      "The reasoning is of high quality because it:\n",
      "\n",
      "1. **Accurately interprets the text**: The reasoning correctly understands the phrase \"Pricing information is available\" to imply that details about pricing are mentioned, even if the specifics (e.g., numbers) are not provided.\n",
      "2. **Addresses the claim directly**: The reasoning focuses on the explicit mention of pricing information, directly countering any potential assumption that pricing is entirely unmentioned.\n",
      "3. **Avoids over-interpretation**: The reasoning does not read beyond what is explicitly stated, sticking to the text's literal meaning without making unfounded inferences.\n",
      "4. **Demonstrates logical clarity**: The explanation is straightforward, easy to follow, and logically sound, making it a robust justification for its conclusion.\n",
      "Correct: True\n",
      "\n",
      "Paragraph: The company launched its new software with features like data encryption and automatic backups. Pricing information is available, but there's no mention of customer support options.\n",
      "Question: Is customer support discussed?\n",
      "Ground Truth: no\n",
      "LLM Answer: no\n",
      "Reasoning: The paragraph explicitly states \"there's no mention of customer support options\", indicating that customer support is not discussed.\n",
      "Ambiguity Label: clear\n",
      "Ambiguity Reasoning: d classify the question's clarity in relation to the paragraph as **'Clear'**. Here's my reasoning:\n",
      "\n",
      "1. **Directly related topic**: The question is directly related to the content of the paragraph, which discusses the launch of new software and mentions specific aspects of it (features and pricing).\n",
      "2. **Unambiguous terminology**: The term \"customer support\" is used in the question, which has a clear and specific meaning. There's little room for misinterpretation of what is being asked.\n",
      "3. **Paragraph provides a direct answer**: Although the paragraph does not provide detailed information about customer support, it explicitly states that \"there's no mention of customer support options.\" This directly answers the question in a negative light, indicating that customer support is indeed discussed (albeit to state its absence).\n",
      "4. **No contextual assumptions needed**: To answer the question, one doesn't need to make assumptions about the context beyond what's provided in the paragraph. The question can be answered based solely on the information given.\n",
      "\n",
      "Given these points, the question is straightforward and directly answerable based on the paragraph's content, making its clarity in relation to the paragraph **'Clear'**.\n",
      "Quality Grade: 5\n",
      "Quality Justification: (Excellent)**\n",
      "\n",
      "**Justification:**\n",
      "The reasoning is rated as Excellent for several reasons:\n",
      "\n",
      "1. **Direct Evidence**: The reasoning directly quotes the paragraph, ensuring the interpretation is grounded in explicit text.\n",
      "2. **Clear Inference**: The conclusion logically follows from the quoted text, leaving little room for misinterpretation.\n",
      "3. **Avoids Assumptions**: The reasoning does not make assumptions beyond what is stated; it merely acknowledges the absence of information as explicitly noted in the paragraph.\n",
      "4. **Concise and Transparent**: The explanation is brief, straightforward, and easy to understand, making the thought process behind the conclusion clear.\n",
      "Correct: True\n",
      "\n",
      "Paragraph: The company launched its new software with features like data encryption and automatic backups. Pricing information is available, but there's no mention of customer support options.\n",
      "Question: Does it talk about data encryption?\n",
      "Ground Truth: yes\n",
      "LLM Answer: yes\n",
      "Reasoning: The paragraph explicitly mentions \"data encryption\" as one of the features of the company's new software, directly answering the question in the affirmative.\n",
      "Ambiguity Label: clear\n",
      "Ambiguity Reasoning: d classify the question's clarity in relation to the paragraph as **'Clear'**. Here's my reasoning:\n",
      "\n",
      "1. **Direct Mention**: The paragraph explicitly mentions \"data encryption\" as one of the features of the new software. This direct mention establishes a clear connection between the question and the paragraph.\n",
      "2. **Unambiguous Term**: The term \"data encryption\" is specific and has a widely understood meaning, reducing the likelihood of misinterpretation.\n",
      "3. **Simple Yes/No Answer**: The question can be answered with a simple \"yes\" or \"no\" based on the paragraph's content, which further indicates clarity. In this case, the answer would be \"yes\", it does talk about data encryption.\n",
      "\n",
      "Overall, the question directly references a specific detail from the paragraph, making it easy to understand and answer, hence classified as **'Clear'**.\n",
      "Quality Grade: 1\n",
      "Quality Justification: (Poor)**\n",
      "\n",
      "**Justification:** The reasoning provided does not align with the content of the paragraph. The paragraph discusses the launch of new software, its features (data encryption and automatic backups), and the availability of pricing information, but lacks mention of customer support options. However, the question being referenced in the reasoning is not provided, but based on the context, it seems the reasoning is attempting to address a question about customer support or perhaps the existence of a specific feature not related to the one mentioned (data encryption). The reasoning fails to address the actual content of the paragraph in a meaningful way, especially since there's no question provided to evaluate the \"affirmative\" answer to. Effective reasoning should clearly connect the provided information to the question at hand, which is not achieved here.\n",
      "Correct: True\n",
      "\n",
      "Paragraph: The smartphone boasts a 6.5-inch display and comes with a dual-lens camera system. However, the battery capacity and processor details were not revealed.\n",
      "Question: Is the display size mentioned?\n",
      "Ground Truth: yes\n",
      "LLM Answer: yes\n",
      "Reasoning: The paragraph explicitly states \"The smartphone boasts a 6.5-inch display\", directly mentioning the display size.\n",
      "Ambiguity Label: clear\n",
      "Ambiguity Reasoning: sification:** **Clear**\n",
      "\n",
      "**Reasoning:**\n",
      "\n",
      "1. **Direct Mention**: The paragraph directly mentions the display size, stating \"a 6.5-inch display\", which immediately addresses the question.\n",
      "2. **Specificity Match**: The question asks for the display size, and the paragraph provides a specific measurement (6.5 inches), ensuring there's no confusion about what aspect of the display the question refers to.\n",
      "3. **Unambiguous Language**: Both the paragraph and the question use straightforward language, leaving no room for interpretation regarding what is being asked or what information is being provided.\n",
      "4. **Clear Context**: The context of the question (details about the smartphone) is perfectly aligned with the information given in the paragraph, further reducing any potential for ambiguity.\n",
      "\n",
      "Given these points, the question's clarity in relation to the paragraph is unmistakable, leading to a classification of \"Clear\".\n",
      "Quality Grade: 5\n",
      "Quality Justification: (Excellent)**\n",
      "\n",
      "**Justification:**\n",
      "\n",
      "* The reasoning is **directly supported by the text**: it accurately quotes the relevant sentence from the paragraph.\n",
      "* The reasoning is **clear and concise**: it explicitly states what the paragraph mentions about the display size.\n",
      "* There is **no unwarranted assumption or inference**: the conclusion is strictly based on the information provided in the paragraph, without introducing external information or interpretation.\n",
      "* The language used is **objective and neutral**, simply stating the fact as presented in the paragraph, which adds to the overall quality of the reasoning.\n",
      "Correct: True\n",
      "\n",
      "Paragraph: The smartphone boasts a 6.5-inch display and comes with a dual-lens camera system. However, the battery capacity and processor details were not revealed.\n",
      "Question: Does it discuss battery capacity?\n",
      "Ground Truth: no\n",
      "LLM Answer: no\n",
      "Reasoning: Although the paragraph mentions \"battery capacity\", it explicitly states that the details regarding battery capacity \"were not revealed\", implying that the discussion does not provide any information about it, only acknowledging its absence.\n",
      "Ambiguity Label: clear\n",
      "Ambiguity Reasoning: d classify the question's clarity in relation to the paragraph as **Clear**. Here's my reasoning:\n",
      "\n",
      "1. **Directly related to paragraph content**: The question specifically inquires about \"battery capacity\", which is explicitly mentioned in the paragraph.\n",
      "2. **Unambiguous terminology**: The term \"battery capacity\" is a well-defined and widely understood concept in the context of smartphones, leaving little room for misinterpretation.\n",
      "3. **Paragraph provides a direct answer (by implication)**: Although the paragraph doesn't provide a numerical value for battery capacity, it clearly states that the details \"were not revealed\". This implies a direct answer to the question: \"No, the paragraph does not discuss (i.e., provide information about) battery capacity.\"\n",
      "4. **No potential for confusion with other concepts**: The question doesn't use vague terms or phrasing that could be confused with other aspects of the paragraph, such as the camera system or display size.\n",
      "\n",
      "Overall, the question is concise, directly related to the paragraph's content, and uses unambiguous terminology, making it a **Clear** question in relation to the paragraph.\n",
      "Quality Grade: 5\n",
      "Quality Justification: (Excellent)**\n",
      "\n",
      "**Justification:**\n",
      "\n",
      "1. **Accurate interpretation**: The reasoning accurately interprets the paragraph's statement about battery capacity, recognizing the distinction between mentioning a topic and providing substantial information about it.\n",
      "2. **Clear understanding of implications**: The reasoning clearly understands the implication of the phrase \"were not revealed\", which is to indicate the absence of specific details.\n",
      "3. **Logical conclusion**: The conclusion logically follows from the analysis, correctly summarizing the nature of the information (or lack thereof) provided in the paragraph.\n",
      "4. **Concise expression**: The reasoning is concisely expressed, directly addressing the key points without unnecessary elaboration.\n",
      "5. **No apparent flaws or biases**: The reasoning appears objective, focusing solely on the textual analysis without introducing external assumptions or biases.\n",
      "Correct: True\n",
      "\n",
      "Paragraph: The smartphone boasts a 6.5-inch display and comes with a dual-lens camera system. However, the battery capacity and processor details were not revealed.\n",
      "Question: Are camera features included?\n",
      "Ground Truth: yes\n",
      "LLM Answer: yes\n",
      "Reasoning: The paragraph explicitly mentions that the smartphone \"comes with a dual-lens camera system\", which confirms the inclusion of camera features.\n",
      "Ambiguity Label: clear\n",
      "Ambiguity Reasoning: sification:** **Clear**\n",
      "\n",
      "**Reasoning:**\n",
      "\n",
      "1. **Directly Answerable**: The question \"Are camera features included?\" can be directly answered with a simple \"yes\" or \"no\" based on the information provided in the paragraph.\n",
      "2. **Relevant Information Provided**: The paragraph explicitly mentions a \"dual-lens camera system,\" which clearly indicates the presence of camera features, making the answer to the question unambiguous.\n",
      "3. **No Room for Interpretation**: The question does not require any inference or assumption beyond what is stated. The mention of a \"dual-lens camera system\" unequivocally confirms the inclusion of camera features, leaving no ambiguity in the response.\n",
      "\n",
      "Therefore, given the straightforward nature of the question and the explicit information provided in the paragraph, the question's clarity in relation to the paragraph is classified as 'Clear'.\n",
      "Quality Grade: 5\n",
      "Quality Justification: (Excellent)**\n",
      "\n",
      "**Justification:**\n",
      "\n",
      "* The reasoning directly quotes the relevant part of the paragraph, ensuring accuracy.\n",
      "* It clearly explains how the quoted information supports the conclusion (inclusion of camera features).\n",
      "* The logic is straightforward and easy to follow: \"X is mentioned\" â†’ \"Therefore, X is confirmed to be included\".\n",
      "* There are no assumptions, leaps of logic, or misinterpretations of the original text, making the reasoning robust and reliable. \n",
      "* The conclusion is also modest in its claim, only affirming the presence of a dual-lens camera system, without inferring additional, unsupported details.\n",
      "Correct: True\n",
      "\n",
      "                                model_name  \\\n",
      "0    meta-llama/llama-3.2-3b-instruct:free   \n",
      "1    meta-llama/llama-3.2-3b-instruct:free   \n",
      "2    meta-llama/llama-3.2-3b-instruct:free   \n",
      "3    meta-llama/llama-3.2-3b-instruct:free   \n",
      "4    meta-llama/llama-3.2-3b-instruct:free   \n",
      "5    meta-llama/llama-3.2-3b-instruct:free   \n",
      "6   nvidia/llama-3.1-nemotron-70b-instruct   \n",
      "7   nvidia/llama-3.1-nemotron-70b-instruct   \n",
      "8   nvidia/llama-3.1-nemotron-70b-instruct   \n",
      "9   nvidia/llama-3.1-nemotron-70b-instruct   \n",
      "10  nvidia/llama-3.1-nemotron-70b-instruct   \n",
      "11  nvidia/llama-3.1-nemotron-70b-instruct   \n",
      "\n",
      "                                            paragraph  \\\n",
      "0   The company launched its new software with fea...   \n",
      "1   The company launched its new software with fea...   \n",
      "2   The company launched its new software with fea...   \n",
      "3   The smartphone boasts a 6.5-inch display and c...   \n",
      "4   The smartphone boasts a 6.5-inch display and c...   \n",
      "5   The smartphone boasts a 6.5-inch display and c...   \n",
      "6   The company launched its new software with fea...   \n",
      "7   The company launched its new software with fea...   \n",
      "8   The company launched its new software with fea...   \n",
      "9   The smartphone boasts a 6.5-inch display and c...   \n",
      "10  The smartphone boasts a 6.5-inch display and c...   \n",
      "11  The smartphone boasts a 6.5-inch display and c...   \n",
      "\n",
      "                               question ground_truth llm_answer  \\\n",
      "0              Does it mention pricing?          yes         no   \n",
      "1        Is customer support discussed?           no         no   \n",
      "2   Does it talk about data encryption?          yes        yes   \n",
      "3        Is the display size mentioned?          yes        yes   \n",
      "4     Does it discuss battery capacity?           no         no   \n",
      "5         Are camera features included?          yes        yes   \n",
      "6              Does it mention pricing?          yes        yes   \n",
      "7        Is customer support discussed?           no         no   \n",
      "8   Does it talk about data encryption?          yes        yes   \n",
      "9        Is the display size mentioned?          yes        yes   \n",
      "10    Does it discuss battery capacity?           no         no   \n",
      "11        Are camera features included?          yes        yes   \n",
      "\n",
      "                                            reasoning ambiguity_label  \\\n",
      "0   , it does not mention pricing. The text states...       ambiguous   \n",
      "1   , The paragraph states that pricing informatio...       ambiguous   \n",
      "2   . The paragraph mentions data encryption as on...           clear   \n",
      "3      . The display size is mentioned as \"6.5-inch\".           clear   \n",
      "4   , The paragraph does not explicitly discuss ba...           clear   \n",
      "5   . The paragraph specifically mentions that the...       ambiguous   \n",
      "6   The paragraph explicitly states \"Pricing infor...           clear   \n",
      "7   The paragraph explicitly states \"there's no me...           clear   \n",
      "8   The paragraph explicitly mentions \"data encryp...           clear   \n",
      "9   The paragraph explicitly states \"The smartphon...           clear   \n",
      "10  Although the paragraph mentions \"battery capac...           clear   \n",
      "11  The paragraph explicitly mentions that the sma...           clear   \n",
      "\n",
      "                                  ambiguity_reasoning  quality_grade  \\\n",
      "0   assify the question's clarity as 'Ambiguous'.\\...              4   \n",
      "1   assify the question's clarity as 'Ambiguous'.\\...              4   \n",
      "2   d classify the question's clarity as 'Clear'. ...              4   \n",
      "3   d classify the question's clarity as 'Clear'. ...              4   \n",
      "4   d classify the question's clarity as 'Clear'.\\...              4   \n",
      "5   assify the question as 'Ambiguous'.\\n\\nThe que...              4   \n",
      "6   d classify the question's clarity in relation ...              5   \n",
      "7   d classify the question's clarity in relation ...              5   \n",
      "8   d classify the question's clarity in relation ...              1   \n",
      "9   sification:** **Clear**\\n\\n**Reasoning:**\\n\\n1...              5   \n",
      "10  d classify the question's clarity in relation ...              5   \n",
      "11  sification:** **Clear**\\n\\n**Reasoning:**\\n\\n1...              5   \n",
      "\n",
      "                                quality_justification  is_correct  \n",
      "0   out of 5.\\n\\nThe reasoning is excellent becaus...       False  \n",
      "1   out of 5.\\n\\nThe reasoning is clear and concis...        True  \n",
      "2   .\\n\\nThe reasoning is excellent because it dir...        True  \n",
      "3   out of 5.\\n\\nThe reasoning is clear and concis...        True  \n",
      "4   out of 5.\\n\\nThe reasoning is excellent becaus...        True  \n",
      "5   .\\n\\nThe reasoning is excellent because it ide...        True  \n",
      "6   (Excellent)**\\n\\n**Justification:**\\nThe reaso...        True  \n",
      "7   (Excellent)**\\n\\n**Justification:**\\nThe reaso...        True  \n",
      "8   (Poor)**\\n\\n**Justification:** The reasoning p...        True  \n",
      "9   (Excellent)**\\n\\n**Justification:**\\n\\n* The r...        True  \n",
      "10  (Excellent)**\\n\\n**Justification:**\\n\\n1. **Ac...        True  \n",
      "11  (Excellent)**\\n\\n**Justification:**\\n\\n* The r...        True  \n"
     ]
    }
   ],
   "source": [
    "df_results = evaluate_dataset(models, dataset)\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_combined_score(df):\n",
    "    total_questions = len(df)\n",
    "    correct_answers = df[\"is_correct\"].sum()\n",
    "    accuracy_score = (correct_answers / total_questions) * 100 if total_questions > 0 else 0\n",
    "\n",
    "    clear_questions = (df[\"ambiguity_label\"] == \"clear\").sum()\n",
    "    ambiguity_handling_score = (clear_questions / total_questions) * 100 if total_questions > 0 else 0\n",
    "\n",
    "    avg_quality_grade = df[\"quality_grade\"].mean()\n",
    "    explanation_quality_score = (avg_quality_grade / 5) * 100 if avg_quality_grade > 0 else 0\n",
    "\n",
    "    \n",
    "    combined_score = (\n",
    "        0.5 * accuracy_score\n",
    "        + 0.3 * ambiguity_handling_score\n",
    "        + 0.2 * explanation_quality_score\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy_score\": accuracy_score,\n",
    "        \"ambiguity_handling_score\": ambiguity_handling_score,\n",
    "        \"explanation_quality_score\": explanation_quality_score,\n",
    "        \"combined_score\": combined_score,\n",
    "    }\n",
    "\n",
    "\n",
    "# Filter rows where ambiguity_label is \"ambiguous\" or quality_grade < 3\n",
    "def human_evaluation_check(df):\n",
    "    human_check_df = df[\n",
    "        (df[\"ambiguity_label\"] == \"ambiguous\") | (df[\"quality_grade\"] < 3)\n",
    "    ]\n",
    "    return human_check_df\n",
    "\n",
    "\n",
    "final_results = []\n",
    "\n",
    "for model_name in df_results[\"model_name\"].unique():\n",
    "    model_df = df_results[df_results[\"model_name\"] == model_name]\n",
    "    scores = calculate_combined_score(model_df)\n",
    "    scores[\"model_name\"] = model_name\n",
    "    final_results.append(scores)\n",
    "\n",
    "scores_df = pd.DataFrame(final_results)\n",
    "\n",
    "human_check_df = human_evaluation_check(df_results)\n",
    "\n",
    "best_model = scores_df.sort_values(by=\"combined_score\", ascending=False).iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               model_name  \\\n",
      "0   meta-llama/llama-3.2-3b-instruct:free   \n",
      "1   meta-llama/llama-3.2-3b-instruct:free   \n",
      "5   meta-llama/llama-3.2-3b-instruct:free   \n",
      "8  nvidia/llama-3.1-nemotron-70b-instruct   \n",
      "\n",
      "                                           paragraph  \\\n",
      "0  The company launched its new software with fea...   \n",
      "1  The company launched its new software with fea...   \n",
      "5  The smartphone boasts a 6.5-inch display and c...   \n",
      "8  The company launched its new software with fea...   \n",
      "\n",
      "                              question ground_truth llm_answer  \\\n",
      "0             Does it mention pricing?          yes         no   \n",
      "1       Is customer support discussed?           no         no   \n",
      "5        Are camera features included?          yes        yes   \n",
      "8  Does it talk about data encryption?          yes        yes   \n",
      "\n",
      "                                           reasoning ambiguity_label  \\\n",
      "0  , it does not mention pricing. The text states...       ambiguous   \n",
      "1  , The paragraph states that pricing informatio...       ambiguous   \n",
      "5  . The paragraph specifically mentions that the...       ambiguous   \n",
      "8  The paragraph explicitly mentions \"data encryp...           clear   \n",
      "\n",
      "                                 ambiguity_reasoning  quality_grade  \\\n",
      "0  assify the question's clarity as 'Ambiguous'.\\...              4   \n",
      "1  assify the question's clarity as 'Ambiguous'.\\...              4   \n",
      "5  assify the question as 'Ambiguous'.\\n\\nThe que...              4   \n",
      "8  d classify the question's clarity in relation ...              1   \n",
      "\n",
      "                               quality_justification  is_correct  \n",
      "0  out of 5.\\n\\nThe reasoning is excellent becaus...       False  \n",
      "1  out of 5.\\n\\nThe reasoning is clear and concis...        True  \n",
      "5  .\\n\\nThe reasoning is excellent because it ide...        True  \n",
      "8  (Poor)**\\n\\n**Justification:** The reasoning p...        True  \n",
      "accuracy_score                                                100.0\n",
      "ambiguity_handling_score                                      100.0\n",
      "explanation_quality_score                                 86.666667\n",
      "combined_score                                            97.333333\n",
      "model_name                   nvidia/llama-3.1-nemotron-70b-instruct\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(human_check_df)\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_check_df.to_csv(\"human_check_list.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"model_comparison_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
